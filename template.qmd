---
title: "Paper summary"
author: "Ben Jaques-Leslie"
date: now
date-format: long
format: 
  html:
    code-fold: true
    toc: true
    fig-width: 8
    fig-cap-location: top
    self-contained: true
editor: visual
execute: 
  echo: false
  warning: false
  error: false
bibliography: references.bib
params:
  paper_name: "Obtaining Comparable Measures of Agency Performance: An Application to U.S. Federal Agencies, 2002-2022"
  authors_names: "George A. Krause and David E. Lewis"
  date_of_issue: "June 2023"
---

```{r}

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)

library(tidyverse)
library(kableExtra)
```

```{r format dates}
nice <- stamp("March 1, 1999", quiet = TRUE)
```

```{r prepare params table}
params_tbl <- #as_tibble(params)
  tibble(Name = names(params), 
         Value = c(params)) %>% 
  unnest(cols = c(Value)) %>% 
   mutate(Name = str_replace_all(Name, "_", " "),
         Name = str_to_sentence(Name))
 
```

# Basic information

```{r display params table}
params_tbl %>% 
  knitr::kable("html") %>% 
  kable_styling(full_width = F)
```

# Summary

In "Obtaining Comparable Measures of Agency Performance: An Application to U.S. Federal Agencies, 2002-2022", George A. Krause and David E. Lewis develop a method for measuring Federal agency performance using a Bayesian structural equation measurement (BESM) model. Federal agencies have detailed and extensive performance management systems, but the outputs of these systems (the measurements of performance) are difficult to compare across agencies. The method proposed by the authors creates a new kinds of agency performance metric that uses data from difference levels of agencies and related to different tasks to create an overall metric of performance for federal agencies. This is a draft paper and has not been published.

In this paper, the authors begin by discussing the difficulty in comparing performance across agencies in Federal government. They identify five reasons that comparative performance measurement is difficult:

1.  Tasks are hard to observe and, when compared against other agencies, the tasks are far removed from the mission of the agency

2.  Agencies have sub units that may have varied tasks and objectives

3.  Goals of agencies differ or are unclear

4.  Academics and practitioners evaluate performance differently

5.  Players disagree about what is good performance.

The authors assume that latent agency performance exists and can be measured using observable indicators. To further define, the authors must account for differences between players (Democrats and Republicans) in what good performance is and disentangle tasks from performance. For the first, they state that an agency's good performance is capably doing their job as defined by *legal requirements*. That is not by the preferences of political leaders. For the second, the authors make clear that contributors to performance **are not** measures of performance. We need to define something to measure performance independently.

They propose that a latent agency performance can be measured using subjective and objective data from a variety of sources, using a BESM. They start by aggregating measures of tasks from the sub-unit up to the agency. Further, they aggregate subjective evaluations of performance across different criteria to uncover the latent dimension of performance. The researchers use data on subjective assessments of agency performance from employees (FEVS) as well as objective measures, like number of employees winning awards or congressional investigations.

They perform a two part process. First, the researchers use a **Bayesian Exploratory Factor Analysis (BEFA)** to evaluate the dimensional of the observed data. Second, they employed BESM to produce measures of latent agency performance. BEFA indicated two latent performance dimensions. The first, ***management performance***, consisted of measures of operations core to the enactment of the agencies mission. The second, ***outcome performance***, consisted on the outcomes related to the agency, but was a much noisier measure. The researchers preferred measure is management performance.

The researchers found that the subjective measures of performance were highly correlated to management performance, with low p-values. Outcome performance, on the other hand, has measures that are very imprecisely associated with this dimension, except for the *GAO High Risk Program Count*.

Some agencies are persistently high or low on the management performance measure. NSF, NASA, and the National Regulatory Commission performed highly. DHS, in particular, performed poorly, as did HUD.

The outcome performance measure generated by this process is a promising way to measure agency performance. It incorporates different data sources to produce a single metric of performance that is comparable across agencies. Furthermore, as new data becomes available it may be included to improve the measure.

# Questions

1.  Is it a problem that the management performance measure is driven by subjective performance measures?

2.  The agencies that perform low on the measure aren't surprising. Is this because their goals are contentious? Hard to meet?

3.  Is there a threshold for poor performance? Below zero?

4.  Are these measures a way to judge administrations?

5.  Are the measures comparable through time?

6.  Is it possible to create these measures on an annual basis?

7.  Do the perfomance measure have meaning by themselves? How might you communicate these results?

8.  What's up with the huge variance of SBA?

9.  Is there a relationship between the clarity of an agencies' mission and performance?

10. Does incorporating new data as it becomes available make the measure less backwards compatible?

@paper

# Citation

::: {#refs}
:::

<!-- Suggests from https://dianaribeiro.com/summary-research-paper/ -->

<!-- THREE STEPS TO SUMMARISE A RESEARCH PAPER -->

<!-- 1. SCAN AND EXTRACT THE MAIN POINTS -->

<!-- First things first, so you have to read the paper. But that doesn’t mean you have to read it from start to finish. Start by scanning the article for its main points. -->

<!-- Here’s the essential information to extract from the research paper you have in front of you: -->

<!-- Authors, year, doi -->

<!-- Study question: look in the introduction for a phrase like “the aim of this study was” -->

<!-- Hypothesis tested -->

<!-- Study methods: design, participants, materials, procedure, what was manipulated (independent variables), what was measured (dependent variables), how data were analysed. -->

<!-- Findings: from the results section; fill this before you look at the discussion section, if possible. Write bullet points. -->

<!-- Interpretation: how did the authors interpreted their findings? Use short sentences, in your own words. -->

<!-- After extracting the key information , revisit the article and read it more attentively, to see if you missed something. Add some notes to your summary, but take care to avoid plagiarism. Write notes in your own words. If you can’t do that at this moment, use quotation marks to indicate that your note came straight from the study. You can rewrite it later, when you have a better grasp of the study. -->

<!-- 2. USE A JOURNALISTIC APPROACH FOR THE FIRST DRAFT -->

<!-- Some sources advise you to keep the same structure as the scientific article, but I like to use the journalistic approach of news articles and flush out the more relevant information first, followed by the details. This is more enticing for readers, making them want to continue reading. Yes, I know that your reader may be just you, but I know I have lost myself in some of the things I’ve written, so…keep it interesting, even for a future self 😊. -->

<!-- This is the main information you have to put together: -->

<!-- Title of the article: I like to keep the original article title for the summary, because it’s easier to refer back to the original article if I need to. Sometimes I add a second title, just for me, if the article title is too obscure or long. -->

<!-- 1st paragraph: Answer the 5 W’s in 3-4 sentences. -->

<!-- Who? (the authors) -->

<!-- What? (main finding) -->

<!-- When and where? (journal, date of publication) -->

<!-- Why? (relevance) -->

<!-- This should be a standalone paragraph, meaning that the reader should be able to take out the main information even if they just read this paragraph. -->

<!-- Subsequent paragraphs: In 2-3 paragraphs or less, provide context and more information about the research done. If you’re not sure if a detail is important or not, you can include it here and edit it out in the next step. -->

<!-- 3. POLISH THE ROUGH EDGES -->

<!-- In this stage, you’re going to make a quick edit, checking for completeness and accuracy. Make sure you’ve included all the main points without repeating yourself. Double-check all the numbers. Stay focused on the research questions to avoid tangents. Avoid using jargon and the passive voice whenever possible. -->

<!-- FINAL SUMMARY -->

<!-- Using this approach, you’ll end up with a short summary of your article that you can use to craft other types of writing, such as press releases, news articles, social media blurbs, and many others. -->

<!-- The advantages of summarising research articles are that you can better understand what the article is about, and you’ll have a text written by you, so it’s easier to adapt and you avoid unintentional plagiarism. -->

<!-- That’s it! My guide to write a research paper summary 😊 -->

<!-- I’ve created a handout with all the information in this blog post plus a fill-in-the-blanks template that you can use to summarise research articles, you can download it using the form below. You’ll be signed up to my mailing list, and receive a weekly roundup of news in the biomedical industry as a bonus! -->

<!-- Examples of .bib file entries can be found https://github.com/plk/biblatex/blob/dev/bibtex/bib/biblatex/biblatex-examples.bib and  https://web.mit.edu/rsi/www/pdfs/bibtex-format.pdf and https://www.bibtex.com/format/ -->
